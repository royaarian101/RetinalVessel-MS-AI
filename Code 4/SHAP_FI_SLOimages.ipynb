{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1E4Z5lB_ByamtFGyhTxbbAY3yPiEMHwAS","timestamp":1721627863237},{"file_id":"103xCuhlcEEy6s2dDyLHDKpSsby9GCHs5","timestamp":1721560634736},{"file_id":"1PxOXnt1WwqQgJlCTT7sbQsbxlK3D4ew2","timestamp":1721554629232}]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"xO0anXlbu4vX"},"outputs":[],"source":["'''\n","measuring the importance of each clinical feature for classifying between MS and HC.\n","The most discriminative features are first selected using the Mann-Whitney U test, a statistical method that identifies significant differences between the two groups.\n","The SHAP (SHapley Additive exPlanations) method is then used to determine feature importance.\n","@author: Asieh Soltanipour, Asieh.soltanipour1365@gmail.com\n","'''\n","%matplotlib inline\n","import matplotlib.pyplot as plt\n","import matplotlib.ticker as plticker\n","from matplotlib.collections import LineCollection\n","import numpy as np\n","import pandas as pd\n","import seaborn as sns\n","import pickle\n","from operator import itemgetter\n","from decimal import Decimal\n","from sklearn import svm\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.inspection import permutation_importance\n","from sklearn import metrics\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.metrics import ConfusionMatrixDisplay, roc_curve, precision_recall_curve,confusion_matrix,RocCurveDisplay,accuracy_score,roc_auc_score,auc\n","from sklearn.model_selection import cross_val_score,RepeatedStratifiedKFold,StratifiedKFold\n","from sklearn.manifold import TSNE\n","from xgboost import XGBClassifier\n","from sklearn.model_selection import train_test_split\n"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cnyi11ImvttD","executionInfo":{"status":"ok","timestamp":1721625875862,"user_tz":-210,"elapsed":30490,"user":{"displayName":"Asieh Soltanipour","userId":"06671393458409262845"}},"outputId":"52c967f2-5cbc-47c7-fc15-00028aeab3ec"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["def preparing(x, y):\n","\n","     data  = []\n","     label = []\n","     for i in x:\n","         for j in range(len(x[i])):\n","             data.append(x[i][j])\n","             label.append(y[i])\n","\n","     data = np.reshape(data, np.shape(data))\n","     #normalizing data\n","     for k in range(data.shape[1]):\n","         q=np.where(data[:,k] != -1)[0]\n","         data[q,k]=(data[q,k]-data[q,k].min())/(data[q,k].max()-data[q,k].min())\n","     return data, label\n"],"metadata":{"id":"vYfob8UWEfDm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def metrics_calculation(y_valid, y_pred, y_prob):\n","\n","    #####################################################\n","    #Get the confusion matrix\n","    #####################################################\n","    ROC_AUC = roc_auc_score(y_valid, y_prob)\n","\n","    f1 = metrics.f1_score(y_valid, y_pred, average='weighted')\n","    precision, recall, thresholds = precision_recall_curve(y_valid, y_prob)\n","    P_R_AUC = auc(recall, precision)\n","    cm = confusion_matrix(y_valid, y_pred)\n","    cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n","    class_acc = cm.diagonal()\n","    Specificity = cm[0,0]/(cm[0,0]+cm[0,1])\n","    Sensitivity = cm[1,1]/(cm[1,0]+cm[1,1])\n","    Precision   = cm[1,1]/(cm[0,1]+cm[1,1])\n","    return Specificity, Sensitivity, Precision, f1, ROC_AUC, P_R_AUC, class_acc, cm"],"metadata":{"id":"DKYxMt1vEatO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","def fold_curves(ax, model, x_valid, y_valid, fold_number, mean_fpr, tprs=[], aucs=[]):\n","    ############ ROC Curve\n","\n","    viz = RocCurveDisplay.from_estimator(\n","        model,\n","        x_valid,\n","        y_valid,\n","        name=\"ROC Curve fold {}\".format(fold_number),\n","        alpha=0.3,\n","        lw=1,\n","        ax=ax,\n","    )\n","    interp_tpr = np.interp(mean_fpr, viz.fpr, viz.tpr)\n","    interp_tpr[0] = 0.0\n","    tprs.append(interp_tpr)\n","    aucs.append(viz.roc_auc)\n","    return tprs, aucs"],"metadata":{"id":"GAjN-_9WEg-M"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["'''\n","loading training SLO features:\n","features_SLO is a dictionary in which its keys is the number of subjects and the corresponding value for each key in an array with size of (number of images per subject X number of features).\n","labels_SLO is a dictionary in which its  key defined the nember of subject and its corresponding value is lable of each subject(MS and HC)\n","'''\n","features_SLO=pickle.load(open('/content/drive/MyDrive/'+'features_SLO'+'.pkl', 'rb'))\n","labels_SLO=pickle.load(open('/content/drive/MyDrive/'+'labels_SLO'+'.pkl', 'rb'))"],"metadata":{"id":"lLKqyrzVIX9N"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["'''\n","applying a statistical-based filter method to our dataset for feature selection (FS),\n","dividing features_SLO into MS and HC groups and then computing the mean values of each feature across all IR-SLO images for individual subjects.\n","Given the non-normal distribution of clinical features, we employed the Mann-Whitney U test to identify significant features differentiating these groups\n","'''\n","HC_features=[]\n","MS_features=[]\n","for i in list(labels_SLO.keys()):\n","  #HC\n","  if labels_SLO[i] == 0:\n","    HC_features.append(np.mean(features_SLO[i],axis=0))\n","  if labels_SLO[i] == 1:\n","    MS_features.append(np.mean(features_SLO[i],axis=0))\n","HC_features=np.array(HC_features)\n","MS_features=np.array(MS_features)\n","\n","from scipy.stats import mannwhitneyu\n","statistic, p_value = mannwhitneyu(HC_features, MS_features, alternative='two-sided')\n","# Using a significance level set at p < 0.05 to identify discriminating features between HC and MS individuals\n","indices = np.where(p_value < 0.05)[0]\n"],"metadata":{"id":"5WDdgNoJ7Pm6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["'''\n","Before building training and validation data  sets using k-fold cross-validation , it is necessary to creat an internal test.\n","To create the internal test dataset, you can use a subject-wise spliting or a stochastic matching method based on age and gender.\n","\n","In the latter method, initially, 20% of subjects with MS were randomly chosen and designated as the internal test dataset. For each selected MS case, an HC patient with the closest\n","  age and the same gender was also included in the age-gender matching test dataset.\n","\n","after creating internal test, you should consider features with p_value<0.05\n","\n","'''\n","# Separate subjects by label\n","hc_subjects = [subj for subj, label in labels_SLO.items() if label == 0]\n","ms_subjects = [subj for subj, label in labels_SLO.items() if label == 1]\n","\n","# Define split ratio (e.g., 80% training, 20% test)\n","train_ratio = 0.8\n","\n","# Shuffle and split each group into training and test sets\n","hc_train, hc_test = train_test_split(hc_subjects, train_size=train_ratio, random_state=42)\n","ms_train, ms_test = train_test_split(ms_subjects, train_size=train_ratio, random_state=42)\n","\n","# Combine training and test subjects\n","train_subjects = hc_train + ms_train\n","test_subjects = hc_test + ms_test\n","\n","# Prepare training and test data and labels\n","train_data = {subj: features_SLO[subj] for subj in train_subjects}\n","internal_test_data = {subj: features_SLO[subj] for subj in test_subjects}\n","\n","train_labels = {subj: labels_SLO[subj] for subj in train_subjects}\n","internal_test_labels = {subj: labels_SLO[subj] for subj in test_subjects}\n","\n","test,label_test = preparing(internal_test_data,internal_test_labels)\n","FS_test = test[:, indices]"],"metadata":{"id":"9ADcnIIO5i7j"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["'''\n","Applying k-fold cross-validation for spliting train_data into training and validation  and considering features with p_value<0.05:\n","\n","'''\n","\n","from sklearn.model_selection import StratifiedKFold\n","skf = StratifiedKFold (n_splits = 5, shuffle = True, random_state = None)\n","nfold = 5  #please enter number of folds\n","kf_nfold = StratifiedKFold(n_splits=nfold, random_state=None, shuffle=True)\n","n = 0\n","x_train_folds={}\n","x_valid_folds={}\n","y_train_folds={}\n","y_valid_folds={}\n","for train_index, val_index in kf_nfold.split(train_data,list(train_labels.values())):\n","\n","    train_index, val_index = next (skf.split (train_data, list(train_labels.values())))\n","    x_train = {i: train_data[list(train_data.keys())[i]]  for i in train_index}\n","    x_valid = {i: train_data[list(train_data.keys())[i]]  for i in val_index}\n","    y_train = {i: train_labels[list(train_labels.keys())[i]] for i in train_index}\n","    y_valid = {i: train_labels[list(train_labels.keys())[i]] for i in val_index}\n","    x_train,y_train = preparing(x_train,y_train)\n","    x_valid,y_valid = preparing(x_valid,y_valid)\n","    FS_x_train = x_train[:, indices]\n","    FS_x_valid = x_valid[:, indices]\n","    x_train_folds[n]=FS_x_train\n","    y_train_folds[n]=y_train\n","    x_valid_folds[n]=FS_x_valid\n","    y_valid_folds[n]=y_valid\n","    n = n+1\n","\n","\n","\n","'''loading external test data :'''\n","features_SLO__external_test=pickle.load(open('/content/drive/MyDrive/'+'features_SLO_external_test'+'.pkl', 'rb'))\n","labels_SLO_external_test=pickle.load(open('/content/drive/MyDrive/'+'labels_SLO_external_test'+'.pkl', 'rb'))\n","###applyting prepreing and selectiong feature with p_value<0.05\n","test_external,label_test_external = preparing(features_SLO__external_test,labels_SLO_external_test)\n","FS_test_external = test_external[:, indices]\n"],"metadata":{"id":"rvKtrrpZIh3B"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["'''\n"," SVM classifier to classify IR-SLO images into 2 classes: Ms and HC\n"," we updated SVM hyperparameters for linear, RBF, sigmoid and poly kernels by using Optuna hyperparameters software\n"," '''\n","acc_lin, acc_rbf, acc_sig  = [],[],[]\n","sp_lin, sp_rbf, sp_sig  = [],[],[]\n","se_lin, se_rbf, se_sig  = [],[],[]\n","pr_lin, pr_rbf, pr_sig  = [],[],[]\n","f1_lin, f1_rbf, f1_sig   = [],[],[]\n","auc_lin, auc_rbf,auc_sig   = [],[],[]\n","pr_auc_lin, pr_auc_rbf, pr_auc_sig   = [],[],[]\n","class_acc_rbf=np.zeros((2))\n","number_class=2\n","confusion_matrix_rbf=np.zeros((number_class, number_class))\n","\n","\n","for n_fold in range(5):\n","\n","    x= x_train_folds[n_fold]\n","    y= y_train_folds[n_fold]\n","\n","    linear = svm.SVC(class_weight =None, probability=True,C= 4025.7688422856236,kernel='linear').fit(x,y)\n","    rbf = svm.SVC(class_weight =None, probability=True,C= 7072.776159189032,gamma=3.9196303352470423 ,kernel='rbf').fit(x,y)\n","    sig = svm.SVC(class_weight =None, probability=True,C=7293.275917107514,gamma=0.027443435885140987 ,kernel='sigmoid').fit(x,y)\n","\n","    ACC_l = linear.score(x_valid_folds[n_fold],y_valid_folds[n_fold])\n","    ACC_r = rbf.score(x_valid_folds[n_fold],y_valid_folds[n_fold])\n","    ACC_s = sig.score(x_valid_folds[n_fold],y_valid_folds[n_fold])\n","\n","\n","    linear_pred_val = linear.predict(x_valid_folds[n_fold])\n","    rbf_pred_val = rbf.predict(x_valid_folds[n_fold])\n","    sig_pred_val = sig.predict(x_valid_folds[n_fold])\n","\n","    linear_proba = linear.predict_proba(x_valid_folds[n_fold])[:,1]\n","    rbf_proba    = rbf.predict_proba(x_valid_folds[n_fold])[:,1]\n","    sig_proba    = sig.predict_proba(x_valid_folds[n_fold])[:,1]\n","\n","    SP_l, SE_l, PR_l, f1_l, ROC_AUC_l, P_R_AUC_l, class_acc_l, cm_l = metrics_calculation(y_valid_folds[n_fold], linear_pred_val, linear_proba)\n","    SP_r, SE_r, PR_r, f1_r, ROC_AUC_r, P_R_AUC_r, class_acc_r, cm_r = metrics_calculation(y_valid_folds[n_fold], rbf_pred_val, rbf_proba)\n","    SP_s, SE_s, PR_s, f1_s, ROC_AUC_s, P_R_AUC_s, class_acc_s, cm_s = metrics_calculation(y_valid_folds[n_fold], sig_pred_val, sig_proba)\n","\n","    class_acc_rbf  = np.add(class_acc_rbf,class_acc_r)\n","    confusion_matrix_rbf = np.add(confusion_matrix_rbf,cm_r)\n","\n","    acc_lin.append(ACC_l)\n","    acc_rbf.append(ACC_r)\n","    acc_sig.append(ACC_s)\n","\n","    sp_lin.append(SP_l)\n","    sp_rbf.append(SP_r)\n","    sp_sig.append(SP_s )\n","\n","    se_lin.append(SE_l)\n","    se_rbf.append(SE_r)\n","    se_sig.append(SE_s)\n","\n","    pr_lin.append(PR_l)\n","    pr_rbf.append(PR_r)\n","    pr_sig.append(PR_s)\n","\n","    f1_lin.append(f1_l)\n","    f1_rbf.append(f1_r)\n","    f1_sig.append(f1_s)\n","\n","    auc_lin.append(ROC_AUC_l)\n","    auc_rbf.append(ROC_AUC_r)\n","    auc_sig.append(ROC_AUC_s)\n","\n","    pr_auc_lin.append(P_R_AUC_l)\n","    pr_auc_rbf.append(P_R_AUC_r)\n","    pr_auc_sig.append(P_R_AUC_s)\n","\n","\n","print('****svm with linear***** :')\n","print(f' acc : {np.mean(acc_lin)}, sp : {np.mean(sp_lin)}, se : {np.mean(se_lin)}, pr : {np.mean(pr_lin)},f1 : {np.mean(f1_lin)}, auc : {np.mean(auc_lin)}, pr_auc : {np.mean(pr_auc_lin)} ')\n","\n","print('****svm with rbf***** :')\n","print(f' acc : {np.mean(acc_rbf)}, sp : {np.mean(sp_rbf)}, se : {np.mean(se_rbf)}, pr : {np.mean(pr_rbf)}, f1 : {np.mean(f1_rbf)}, auc : {np.mean(auc_rbf)}, pr_auc : {np.mean(pr_auc_rbf)}')\n","class_acc_rbf  = class_acc_rbf/5\n","print(f' class ACC : {class_acc_rbf}')\n","confusion_matrix_rbf  = confusion_matrix_rbf/5\n","print(f' confusion_matrix : {confusion_matrix_rbf}')\n","\n","print('****svm with sig***** :')\n","print(f' acc : {np.mean(acc_sig)}, sp : {np.mean(sp_sig)}, se : {np.mean(se_sig)}, pr : {np.mean(pr_sig)}, f1 : {np.mean(f1_sig)}, auc : {np.mean(auc_sig)}, pr_auc : {np.mean(pr_auc_sig)} ')\n","\n","\n","target_names = ['Normal' , 'MS']\n","disp = ConfusionMatrixDisplay(confusion_matrix=confusion_matrix_rbf, display_labels=target_names)\n","disp.plot()\n","plt.title('SVM classifier (kernel : RBF)')\n"],"metadata":{"id":"dKL-y8Z9ElcK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["'''using Random Forest classifier :\n","'''\n","acc_rf,sp_rf, se_rf, pr_rf,f1_rf, auc_rf, pr_auc_rf, tprs_rf, aucs_rf, y_pred_rf = [],[],[],[],[],[],[],[],[],[]\n","class_acc_rf=np.zeros((2))\n","number_class=2\n","confusion_matrix_rf=np.zeros((number_class, number_class))\n","\n","for n_fold in range(5):\n","\n","    x=x_train_folds[n_fold]\n","    y=y_train_folds[n_fold]\n","    model= RandomForestClassifier(\n","        random_state=42,max_depth=10,n_estimators=147,min_samples_split=10,min_samples_leaf=2,max_features='sqrt',criterion='gini'\n","        )\n","    model.fit(x,y)\n","    valid=x_valid_folds[n_fold]\n","    ACC_rf=model.score(valid,y_valid_folds[n_fold])\n","    rf_pred_val = model.predict(valid)\n","    rf_proba = model.predict_proba(valid)[:,1]\n","    SP_rf, SE_rf, PR_rf, f1, ROC_AUC_rf, P_R_AUC_rf, class_acc_rf0, cm_rf = metrics_calculation(y_valid_folds[n_fold], rf_pred_val, rf_proba)\n","\n","    acc_rf.append(ACC_rf)\n","    sp_rf.append(SP_rf)\n","    se_rf.append(SE_rf)\n","    pr_rf.append(PR_rf)\n","    f1_rf.append(f1)\n","    auc_rf.append(ROC_AUC_rf)\n","    pr_auc_rf.append(P_R_AUC_rf)\n","    class_acc_rf  = np.add(class_acc_rf,class_acc_rf0)\n","    confusion_matrix_rf = np.add(confusion_matrix_rf,cm_rf)\n","\n","\n","print(f'acc:{np.mean(acc_rf)}')\n","print(f'sp:{np.mean(sp_rf)}')\n","print(f'se:{np.mean(se_rf)}')\n","print(f'pr:{np.mean(pr_rf)}')\n","print(f'f1:{np.mean(f1_rf)}')\n","print(f'auc:{np.mean(auc_rf)}')\n","print(f'pr_auc:{np.mean(pr_auc_rf)}')\n","\n","class_acc_rf  = class_acc_rf/5\n","print(f' class ACC : {class_acc_rf}')\n","confusion_matrix_rf  = confusion_matrix_rf/5\n","print(f' confusion_matrix : {confusion_matrix_rf}')\n","target_names = ['Normal' , 'MS']\n","disp = ConfusionMatrixDisplay(confusion_matrix=confusion_matrix_rf, display_labels=target_names)\n","disp.plot()\n","plt.title('Random Forest Classifier')"],"metadata":{"id":"DbRyPhGeIOpK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["'''\n","XGBoost classifier:\n","'''\n","acc_XGBoost ,sp_XGBoost,se_XGBoost, pr_XGBoost,f1_XGBoost,auc_XGBoost,pr_auc_XGBoost,tprs_XGBoost,aucs_XGBoost,y_pred_XGBoost= [],[],[],[],[],[],[],[],[],[]\n","class_acc_xgb=np.zeros((2))\n","number_class=2\n","confusion_matrix_xgb=np.zeros((number_class, number_class))\n","\n","\n","param = {\n","    \"verbosity\": 0,\n","    \"objective\": \"binary:logistic\",\n","    \"tree_method\": \"auto\",\n","    \"booster\" : \"gbtree\",\n","    \"lambda\" :  0.6262819848273675,\n","    \"alpha\": 0.002417237009326353,\n","    \"subsample\": 0.6816657652094626,\n","    \"colsample_bytree\" :  0.3288592429227775,\n","    \"max_depth\" : 3,\n","    \"min_child_weight\" : 4,\n","    \"eta\":0.02168530523973617,\n","    \"gamma\":  0.01720205835153038,\n","    \"grow_policy\": \"depthwise\",\n","}\n","\n","\n","for n_fold in range(5):\n","\n","    x=x_train_folds[n_fold]\n","    y=y_train_folds[n_fold]\n","    valid=x_valid_folds[n_fold]\n","\n","    model= XGBClassifier(**param)\n","    model.fit(x,y)\n","    ACC_XGBoost=model.score(valid,y_valid_folds[n_fold])\n","    XGBoost_pred_val = model.predict(valid)\n","    XGBoost_proba = model.predict_proba(valid)[:,1]\n","    SP, SE, PR, f1, ROC_AUC, P_R_AUC, class_acc, cm = metrics_calculation(y_valid_folds[n_fold], XGBoost_pred_val, XGBoost_proba)\n","\n","    acc_XGBoost.append(ACC_XGBoost)\n","    sp_XGBoost.append(SP)\n","    se_XGBoost.append(SE)\n","    pr_XGBoost.append(PR)\n","    f1_XGBoost.append(f1)\n","    auc_XGBoost.append(ROC_AUC)\n","    pr_auc_XGBoost.append(P_R_AUC)\n","    class_acc_xgb  = np.add(class_acc_xgb,class_acc)\n","    confusion_matrix_xgb = np.add(confusion_matrix_xgb,cm)\n","\n","print(f'acc: {np.mean(acc_XGBoost)}')\n","print(f'sp: {np.mean(sp_XGBoost)}')\n","print(f'se: {np.mean(se_XGBoost)}')\n","print(f'pr: {np.mean(pr_XGBoost)}')\n","print(f'f: {np.mean(f1_XGBoost)}')\n","print(f'auc: {np.mean(auc_XGBoost)}')\n","print(f'pr_auc: {np.mean(pr_auc_XGBoost)}')\n","class_acc_xgb  = class_acc_xgb/5\n","print(f' class ACC : {class_acc_xgb}')\n","confusion_matrix_xgb  = confusion_matrix_xgb/5\n","print(f' confusion_matrix : {confusion_matrix_xgb}')\n","\n","\n","target_names = ['Normal' , 'MS']\n","disp = ConfusionMatrixDisplay(confusion_matrix=confusion_matrix_xgb, display_labels=target_names)\n","disp.plot()\n","plt.title('XGBoost Classifier')"],"metadata":{"id":"o3lpf5VeKZ-G"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["'''\n"," computing feature importance using SHAP method:\n","'''\n","!pip install shap"],"metadata":{"id":"Wido9cQ3v46b"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#list of the name of selected features by Mann-Whiteny U test\n","#list of manualand clinical features used:\n","features_name=[\n","         #features from segmented disc and cup:\n","        'Disc_width','Cup_width','Disc-Cup widths ratio',\n","         #features from segmented blood vessels in the whole IR-SLO images:\n","        'whole_vessel_width','whole_vessel_density','whole_fractal_dimension','disctance_measure_tortousity','squared_curvature_tortousity',\n","        'tortousity_density','whole_intensity_vessel',\n","         #features from segmented vessels in zone B and zone C:\n","        'vessel_width_Zone B','vessel_width_Zone C','fractal_dimension_zoneB','fractal_dimension_zoneC',\n","        'linear_regression_tortousity_ZoneB','linear_regression_tortousity_ZoneC',\n","        'disctance_measure_tortousity_ZoneB','disctance_measure_tortousity_ZoneC',\n","        'squared_curvature_tortousity_ZoneB','squared_curvature_tortousity_ZoneC',\n","        'tortousity_density_ZoneB','tortousity_density_Zonec',\n","        'vessel_density_zoneB','vessel_density_zoneC',\n","         ]\n","static_selected_features_name = [features_name[i] for i in list(indices)]\n","\n"],"metadata":{"id":"8KB2pEeLLjnR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import shap\n","from shap import Explainer, Explanation\n","\n","def compute_SHAP_importance(x_train_folds,data_inpute,y_input,param,name_data):\n","    list_shap_values = list()\n","    importances_fold={}\n","    for n_fold in range(5):\n","         x=x_train_folds[n_fold]\n","         y=y_train_folds[n_fold]\n","         x_df=pd.DataFrame(x,columns=static_selected_features_name)\n","         model= XGBClassifier(**param,)\n","         model.fit(x_df,y)\n","         explainer = shap.TreeExplainer(model)\n","         if name_data == 'training':\n","            shape_values1=explainer(pd.DataFrame(x, columns=static_selected_features_name))\n","         elif name_data == 'validation':\n","            shape_values1=explainer(pd.DataFrame(data_inpute[n_fold], columns=static_selected_features_name))\n","         elif name_data == 'internal test':\n","            shape_values1=explainer(pd.DataFrame(data_inpute, columns=static_selected_features_name))\n","         else:\n","            shape_values1=explainer(pd.DataFrame(data_inpute, columns=static_selected_features_name))\n","         list_shap_values.append(shape_values1)\n","\n","         importances=[]\n","         for i in range(shape_values1.values.shape[1]):\n","             importances.append(np.mean(np.abs(shape_values1.values[:, i])))\n","         importances_fold[n_fold]=importances\n","\n","    #concatenating shap values for 5 folds:\n","    shap_t_values=np.concatenate((list_shap_values[0].values,list_shap_values[1].values,list_shap_values[2].values,\n","                             list_shap_values[3].values,list_shap_values[4].values),axis=0)\n","    shap_t_base=np.concatenate((list_shap_values[0].base_values,list_shap_values[1].base_values,list_shap_values[2].base_values,\n","                             list_shap_values[3].base_values,list_shap_values[4].base_values),axis=0)\n","\n","    shap_t_data=np.concatenate((list_shap_values[0].data,list_shap_values[1].data,list_shap_values[2].data,\n","                             list_shap_values[3].data,list_shap_values[4].data),axis=0)\n","    ###\n","    '''\n","     as zone B and zone C were not defined in some IR-SLO images, those that do not containing OD,\n","     it is nessesary to normalize shap values by the number of and the number of MS images in which these zones have been defined\n","    '''\n","    if name_data == 'training':\n","         XX=np.concatenate((data_inpute[0],data_inpute[1],data_inpute[2],data_inpute[3],data_inpute[4]),axis=0)\n","         y_tt=y_input[0]+y_input[1]+y_input[2]+y_input[3]+y_input[4]\n","    elif name_data == 'validation':\n","         XX=np.concatenate((data_inpute[0],data_inpute[1],data_inpute[2],data_inpute[3],data_inpute[4]),axis=0)\n","         y_tt=y_input[0]+y_input[1]+y_input[2]+y_input[3]+y_input[4]\n","    else:\n","         xx=data_inpute\n","         y_tt=y_input[0]\n","    XX1=np.copy(XX)\n","    y_tt=np.array(y_tt)\n","    y_tt1=np.copy(y_tt)\n","    values1=np.copy(shap_t_values)\n","    p=np.where(XX1==-1)[0]# number of images in which these zones are not defined\n","    y_tt1=np.delete(y_tt1, list(p), 0)\n","    n_HC=len(np.where(np.array(y_tt1)==0)[0])\n","    n_MS=len(np.where(np.array(y_tt1)==1)[0])\n","   # normalizing shap values and observation\n","    for ii in range(XX.shape[1]):\n","          qq=np.where(XX[:,ii] != -1)[0]\n","          for jj in range(len(qq)):\n","            if y_tt[qq[jj]] == 1 :\n","              XX1[qq[jj],ii]=XX1[qq[jj],ii]/(n_MS/len(y_tt1))\n","              values1[qq[jj],ii]=values1[qq[jj],ii]/(n_MS/len(y_tt1))\n","            if y_tt[qq[jj]] ==0 :\n","              XX1[qq[jj],ii]=XX1[qq[jj],ii]/(n_HC/len(y_tt1))\n","              values1[qq[jj],ii]=values1[qq[jj],ii]/(n_HC/len(y_tt1))\n","\n","    Shap_normalized = Explanation(values1, shap_t_base, shap_t_data , feature_names=static_selected_features_name)\n","    #ploting shap values\n","    #shap.summary_plot(values1,XX1,feature_names=features_name,plot_size=[10,8],show = True, class_names=target_names,max_display=len(features_name))\n","    #shap.plots.bar(Shap_normalized,max_display=len(features_name))\n","    Shap_sumation=Shap_normalized.sum(axis=0)\n","    #shap.plots.bar(Shap_sumation,max_display=len(features_name))\n","    ##\n","\n","  # max_importance_shap : a list includng the order of important features cmputed by SHAP method\n","    importance_fold_shap_external=list(np.mean(np.abs(shap_t_values),axis=0))\n","\n","    shap_importance[name_data]=importance_fold_shap_external\n","\n","    return shap_importance, values1,Shap_normalized\n","\n","shap_importance={}\n","evaluation_list=['training','validation','internal_test','external_test']\n","\n","data=[x_train_folds,x_valid_folds,FS_test,FS_test_external]\n","y_data=[y_train_folds,y_valid_folds,label_test,label_test_external]\n","c=0\n","for i in evaluation_list:\n","    shap_importance, values1,Shap_normalized=compute_SHAP_importance(x_train_folds, data[c],y_data[c] ,param, i)\n","    c += 1\n","\n","\n","cmap = sns.diverging_palette(220, 10, as_cmap=True)\n","u=np.zeros((len(evaluation_list),len(static_selected_features_name)))\n","CC_heat_map=[shap_importance['training'], shap_importance['validation'], shap_importance['internal_test'], shap_importance['external_test']]\n","\n","for i in range(u.shape[0]):\n","  u[i,:]=CC_heat_map[i]\n","df00=pd.DataFrame(index=evaluation_list,data=u,columns=static_selected_features_name)\n","fig, ax = plt.subplots(figsize=(16,4))\n","sns.heatmap(df00,cmap=\"Greens\",linewidths=0.5,ax=ax,annot=True,fmt=\".2f\",annot_kws={\"size\": 11, },linewidth=0.5)"],"metadata":{"id":"DGoOOzCtR5Um"},"execution_count":null,"outputs":[]}]}