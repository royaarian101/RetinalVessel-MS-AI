{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1aj3TtfM37EAgk_yPPwZC24uccEY4wLeA","timestamp":1721627946614},{"file_id":"10XQmAlgO9niZUXwVVfuy9VHyPjC7VAg3","timestamp":1721557490270},{"file_id":"1PxOXnt1WwqQgJlCTT7sbQsbxlK3D4ew2","timestamp":1721554758648}]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"xO0anXlbu4vX"},"outputs":[],"source":["'''\n","this code evaluate the robustness of the proposed algorithm for measuring the importance of clinical features in IR_SLO images for MS and HC classification\n","measuring the importance of each clinical feature for classifying between MS and HC, without considering feature selection.\n","@author: Asieh Soltanipour, Asieh.soltanipour1365@gmail.com\n","'''\n","%matplotlib inline\n","import matplotlib.pyplot as plt\n","import matplotlib.ticker as plticker\n","from matplotlib.collections import LineCollection\n","import numpy as np\n","import pandas as pd\n","import seaborn as sns\n","import pickle\n","from operator import itemgetter\n","from decimal import Decimal\n","from sklearn import svm\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.inspection import permutation_importance\n","from sklearn import metrics\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.metrics import ConfusionMatrixDisplay, roc_curve, precision_recall_curve,confusion_matrix,RocCurveDisplay,accuracy_score,roc_auc_score,auc\n","from sklearn.model_selection import cross_val_score,RepeatedStratifiedKFold,StratifiedKFold\n","from sklearn.manifold import TSNE\n","from xgboost import XGBClassifier\n"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cnyi11ImvttD","executionInfo":{"status":"ok","timestamp":1721554873891,"user_tz":-210,"elapsed":39972,"user":{"displayName":"Asieh Soltanipour","userId":"06671393458409262845"}},"outputId":"46c5cddc-a2b4-4f21-eb80-3a0e313e716b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["def preparing(x, y):\n","\n","     data  = []\n","     label = []\n","     for i in x:\n","         for j in range(len(x[i])):\n","             data.append(x[i][j])\n","             label.append(y[i])\n","\n","     data = np.reshape(data, np.shape(data))\n","     #normalizing data\n","     for k in range(data.shape[1]):\n","         q=np.where(data[:,k] != -1)[0]\n","         data[q,k]=(data[q,k]-data[q,k].min())/(data[q,k].max()-data[q,k].min())\n","     return data, label\n"],"metadata":{"id":"vYfob8UWEfDm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def metrics_calculation(y_valid, y_pred, y_prob):\n","\n","    #####################################################\n","    #Get the confusion matrix\n","    #####################################################\n","    ROC_AUC = roc_auc_score(y_valid, y_prob)\n","\n","    f1 = metrics.f1_score(y_valid, y_pred, average='weighted')\n","    precision, recall, thresholds = precision_recall_curve(y_valid, y_prob)\n","    P_R_AUC = auc(recall, precision)\n","    cm = confusion_matrix(y_valid, y_pred)\n","    cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n","    class_acc = cm.diagonal()\n","    Specificity = cm[0,0]/(cm[0,0]+cm[0,1])\n","    Sensitivity = cm[1,1]/(cm[1,0]+cm[1,1])\n","    Precision   = cm[1,1]/(cm[0,1]+cm[1,1])\n","    return Specificity, Sensitivity, Precision, f1, ROC_AUC, P_R_AUC, class_acc, cm"],"metadata":{"id":"DKYxMt1vEatO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","def fold_curves(ax, model, x_valid, y_valid, fold_number, mean_fpr, tprs=[], aucs=[]):\n","    ############ ROC Curve\n","\n","    viz = RocCurveDisplay.from_estimator(\n","        model,\n","        x_valid,\n","        y_valid,\n","        name=\"ROC Curve fold {}\".format(fold_number),\n","        alpha=0.3,\n","        lw=1,\n","        ax=ax,\n","    )\n","    interp_tpr = np.interp(mean_fpr, viz.fpr, viz.tpr)\n","    interp_tpr[0] = 0.0\n","    tprs.append(interp_tpr)\n","    aucs.append(viz.roc_auc)\n","    return tprs, aucs"],"metadata":{"id":"GAjN-_9WEg-M"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["'''\n","loading training SLO features:\n","features_SLO is a dictionary in which its keys is the number of subjects and the corresponding value for each key in an array with size of (number of images per subject X number of features).\n","labels_SLO is a dictionary in which its  key defined the nember of subject and its corresponding value is lable of each subject(MS and HC)\n","'''\n","features_SLO=pickle.load(open('/content/drive/MyDrive/'+'features_SLO'+'.pkl', 'rb'))\n","labels_SLO=pickle.load(open('/content/drive/MyDrive/'+'labels_SLO'+'.pkl', 'rb'))"],"metadata":{"id":"lLKqyrzVIX9N"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["'''\n","Before building training and validation data  sets using k-fold cross-validation , it is necessary to creat an internal test.\n","To create the internal test dataset, you can use a subject-wise spliting or a stochastic matching method based on age and gender.\n","\n","In the latter method, initially, 20% of subjects with MS were randomly chosen and designated as the internal test dataset. For each selected MS case, an HC patient with the closest\n","  age and the same gender was also included in the age-gender matching test dataset.\n","\n","\n","'''\n","# Separate subjects by label\n","hc_subjects = [subj for subj, label in labels_SLO.items() if label == 0]\n","ms_subjects = [subj for subj, label in labels_SLO.items() if label == 1]\n","\n","# Define split ratio (e.g., 80% training, 20% test)\n","train_ratio = 0.8\n","\n","# Shuffle and split each group into training and test sets\n","hc_train, hc_test = train_test_split(hc_subjects, train_size=train_ratio, random_state=42)\n","ms_train, ms_test = train_test_split(ms_subjects, train_size=train_ratio, random_state=42)\n","\n","# Combine training and test subjects\n","train_subjects = hc_train + ms_train\n","test_subjects = hc_test + ms_test\n","\n","# Prepare training and test data and labels\n","train_data = {subj: features_SLO[subj] for subj in train_subjects}\n","internal_test_data = {subj: features_SLO[subj] for subj in test_subjects}\n","\n","train_labels = {subj: labels_SLO[subj] for subj in train_subjects}\n","internal_test_labels = {subj: labels_SLO[subj] for subj in test_subjects}\n","\n","test,label_test = preparing(internal_test_data,internal_test_labels)\n"],"metadata":{"id":"ZD_AHrM4AXcL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["'''\n","Applying k-fold cross-validation for spliting train_data into training and validation:\n","\n","'''\n","\n","from sklearn.model_selection import StratifiedKFold\n","skf = StratifiedKFold (n_splits = 5, shuffle = True, random_state = None)\n","nfold = 5  #please enter number of folds\n","kf_nfold = StratifiedKFold(n_splits=nfold, random_state=None, shuffle=True)\n","n = 0\n","x_train_folds={}\n","x_valid_folds={}\n","y_train_folds={}\n","y_valid_folds={}\n","for train_index, val_index in kf_nfold.split(train_data,list(train_labels.values())):\n","\n","    train_index, val_index = next (skf.split (train_data, list(train_labels.values())))\n","    x_train = {i: train_data[list(train_data.keys())[i]]  for i in train_index}\n","    x_valid = {i: train_data[list(train_data.keys())[i]]  for i in val_index}\n","    y_train = {i: train_labels[list(train_labels.keys())[i]] for i in train_index}\n","    y_valid = {i: train_labels[list(train_labels.keys())[i]] for i in val_index}\n","    x_train,y_train = preparing(x_train,y_train)\n","    x_valid,y_valid = preparing(x_valid,y_valid)\n","    x_train_folds[n]=x_train\n","    y_train_folds[n]=y_train\n","    x_valid_folds[n]=x_valid\n","    y_valid_folds[n]=y_valid\n","    n = n+1\n","\n","\n","\n","'''loading external test data :'''\n","features_SLO__external_test=pickle.load(open('/content/drive/MyDrive/'+'features_SLO_external_test'+'.pkl', 'rb'))\n","labels_SLO_external_test=pickle.load(open('/content/drive/MyDrive/'+'labels_SLO_external_test'+'.pkl', 'rb'))\n","###applyting prepreing and selectiong feature with p_value<0.05\n","test_external,label_test_external = preparing(features_SLO__external_test,labels_SLO_external_test)\n","\n"],"metadata":{"id":"rvKtrrpZIh3B"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["'''\n","XGBoost classifier:\n","'''\n","acc_XGBoost ,sp_XGBoost,se_XGBoost, pr_XGBoost,f1_XGBoost,auc_XGBoost,pr_auc_XGBoost,tprs_XGBoost,aucs_XGBoost,y_pred_XGBoost= [],[],[],[],[],[],[],[],[],[]\n","class_acc_xgb=np.zeros((2))\n","number_class=2\n","confusion_matrix_xgb=np.zeros((number_class, number_class))\n","\n","\n","param = {\n","    \"verbosity\": 0,\n","    \"objective\": \"binary:logistic\",\n","    \"tree_method\": \"auto\",\n","    \"booster\" : \"gbtree\",\n","    \"lambda\" :  0.6262819848273675,\n","    \"alpha\": 0.002417237009326353,\n","    \"subsample\": 0.6816657652094626,\n","    \"colsample_bytree\" :  0.3288592429227775,\n","    \"max_depth\" : 3,\n","    \"min_child_weight\" : 4,\n","    \"eta\":0.02168530523973617,\n","    \"gamma\":  0.01720205835153038,\n","    \"grow_policy\": \"depthwise\",\n","}\n","\n","\n","for n_fold in range(5):\n","\n","    x=x_train_folds[n_fold]\n","    y=y_train_folds[n_fold]\n","    valid=x_valid_folds[n_fold]\n","\n","    model= XGBClassifier(**param)\n","    model.fit(x,y)\n","    ACC_XGBoost=model.score(valid,y_valid_folds[n_fold])\n","    XGBoost_pred_val = model.predict(valid)\n","    XGBoost_proba = model.predict_proba(valid)[:,1]\n","    SP, SE, PR, f1, ROC_AUC, P_R_AUC, class_acc, cm = metrics_calculation(y_valid_folds[n_fold], XGBoost_pred_val, XGBoost_proba)\n","\n","    acc_XGBoost.append(ACC_XGBoost)\n","    sp_XGBoost.append(SP)\n","    se_XGBoost.append(SE)\n","    pr_XGBoost.append(PR)\n","    f1_XGBoost.append(f1)\n","    auc_XGBoost.append(ROC_AUC)\n","    pr_auc_XGBoost.append(P_R_AUC)\n","    class_acc_xgb  = np.add(class_acc_xgb,class_acc)\n","    confusion_matrix_xgb = np.add(confusion_matrix_xgb,cm)\n","\n","print(f'acc: {np.mean(acc_XGBoost)}')\n","print(f'sp: {np.mean(sp_XGBoost)}')\n","print(f'se: {np.mean(se_XGBoost)}')\n","print(f'pr: {np.mean(pr_XGBoost)}')\n","print(f'f: {np.mean(f1_XGBoost)}')\n","print(f'auc: {np.mean(auc_XGBoost)}')\n","print(f'pr_auc: {np.mean(pr_auc_XGBoost)}')\n","class_acc_xgb  = class_acc_xgb/5\n","print(f' class ACC : {class_acc_xgb}')\n","confusion_matrix_xgb  = confusion_matrix_xgb/5\n","print(f' confusion_matrix : {confusion_matrix_xgb}')\n","\n","\n","target_names = ['Normal' , 'MS']\n","disp = ConfusionMatrixDisplay(confusion_matrix=confusion_matrix_xgb, display_labels=target_names)\n","disp.plot()\n","plt.title('XGBoost Classifier')"],"metadata":{"id":"o3lpf5VeKZ-G"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["'''\n"," computing feature importance using SHAP method:\n","'''\n","!pip install shap"],"metadata":{"id":"Wido9cQ3v46b","executionInfo":{"status":"ok","timestamp":1721555276709,"user_tz":-210,"elapsed":8921,"user":{"displayName":"Asieh Soltanipour","userId":"06671393458409262845"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"3922eb4a-806e-4422-cbf3-bb651c00f1a8"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting shap\n","  Downloading shap-0.46.0-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (540 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m540.1/540.1 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from shap) (1.25.2)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from shap) (1.11.4)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from shap) (1.2.2)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from shap) (2.0.3)\n","Requirement already satisfied: tqdm>=4.27.0 in /usr/local/lib/python3.10/dist-packages (from shap) (4.66.4)\n","Requirement already satisfied: packaging>20.9 in /usr/local/lib/python3.10/dist-packages (from shap) (24.1)\n","Collecting slicer==0.0.8 (from shap)\n","  Downloading slicer-0.0.8-py3-none-any.whl (15 kB)\n","Requirement already satisfied: numba in /usr/local/lib/python3.10/dist-packages (from shap) (0.58.1)\n","Requirement already satisfied: cloudpickle in /usr/local/lib/python3.10/dist-packages (from shap) (2.2.1)\n","Requirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba->shap) (0.41.1)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->shap) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->shap) (2023.4)\n","Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->shap) (2024.1)\n","Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->shap) (1.4.2)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->shap) (3.5.0)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->shap) (1.16.0)\n","Installing collected packages: slicer, shap\n","Successfully installed shap-0.46.0 slicer-0.0.8\n"]}]},{"cell_type":"code","source":["#list of the name of selected features by Mann-Whiteny U test\n","#list of manualand clinical features used:\n","features_name=[\n","         #features from segmented disc and cup:\n","        'Disc_width','Cup_width','Disc-Cup widths ratio',\n","         #features from segmented blood vessels in the whole IR-SLO images:\n","        'whole_vessel_width','whole_vessel_density','whole_fractal_dimension','disctance_measure_tortousity','squared_curvature_tortousity',\n","        'tortousity_density','whole_intensity_vessel',\n","         #features from segmented vessels in zone B and zone C:\n","        'vessel_width_Zone B','vessel_width_Zone C','fractal_dimension_zoneB','fractal_dimension_zoneC',\n","        'linear_regression_tortousity_ZoneB','linear_regression_tortousity_ZoneC',\n","        'disctance_measure_tortousity_ZoneB','disctance_measure_tortousity_ZoneC',\n","        'squared_curvature_tortousity_ZoneB','squared_curvature_tortousity_ZoneC',\n","        'tortousity_density_ZoneB','tortousity_density_Zonec',\n","        'vessel_density_zoneB','vessel_density_zoneC',\n","         ]\n","\n"],"metadata":{"id":"8KB2pEeLLjnR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import shap\n","from shap import Explainer, Explanation\n","\n","def compute_SHAP_importance(x_train_folds,data_inpute,y_input,param,name_data,features_name):\n","    list_shap_values = list()\n","    importances_fold={}\n","    for n_fold in range(5):\n","         x=x_train_folds[n_fold]\n","         y=y_train_folds[n_fold]\n","         x_df=pd.DataFrame(x,columns=features_name)\n","         model= XGBClassifier(**param,)\n","         model.fit(x_df,y)\n","         explainer = shap.TreeExplainer(model)\n","         if name_data == 'training':\n","            shape_values1=explainer(pd.DataFrame(x, columns=features_name))\n","         elif name_data == 'validation':\n","            shape_values1=explainer(pd.DataFrame(data_inpute[n_fold], columns=features_name))\n","         elif name_data == 'internal test':\n","            shape_values1=explainer(pd.DataFrame(data_inpute, columns=features_name))\n","         else:\n","            shape_values1=explainer(pd.DataFrame(data_inpute, columns=features_name))\n","         list_shap_values.append(shape_values1)\n","\n","         importances=[]\n","         for i in range(shape_values1.values.shape[1]):\n","             importances.append(np.mean(np.abs(shape_values1.values[:, i])))\n","         importances_fold[n_fold]=importances\n","\n","    #concatenating shap values for 5 folds:\n","    shap_t_values=np.concatenate((list_shap_values[0].values,list_shap_values[1].values,list_shap_values[2].values,\n","                             list_shap_values[3].values,list_shap_values[4].values),axis=0)\n","    shap_t_base=np.concatenate((list_shap_values[0].base_values,list_shap_values[1].base_values,list_shap_values[2].base_values,\n","                             list_shap_values[3].base_values,list_shap_values[4].base_values),axis=0)\n","\n","    shap_t_data=np.concatenate((list_shap_values[0].data,list_shap_values[1].data,list_shap_values[2].data,\n","                             list_shap_values[3].data,list_shap_values[4].data),axis=0)\n","\n","\n","    Shap_normalized = Explanation(shap_t_values, shap_t_base, shap_t_data , feature_names=features_name)\n","    #ploting shap values\n","    #shap.summary_plot(shap_t_values,XX1,feature_names=features_name,plot_size=[10,8],show = True, class_names=target_names,max_display=len(features_name))\n","    #shap.plots.bar(Shap_normalized,max_display=len(features_name))\n","    Shap_sumation=Shap_normalized.sum(axis=0)\n","    #shap.plots.bar(Shap_sumation,max_display=len(features_name))\n","\n","\n","  # max_importance_shap : a list includng the order of important features cmputed by SHAP method\n","    importance_fold_shap_external=list(np.mean(np.abs(shap_t_values),axis=0))\n","\n","    shap_importance[name_data]=importance_fold_shap_external\n","\n","    return shap_importance\n","\n","shap_importance={}\n","evaluation_list=['training','validation','internal_test','external_test']\n","data=[x_train_folds,x_valid_folds,test,test_external]\n","y_data=[y_train_folds,y_valid_folds,label_test,label_test_external]\n","\n","c=0\n","for i in evaluation_list:\n","    shap_importance=compute_SHAP_importance(x_train_folds, data[c],y_data[c] ,param, i,features_name)\n","    c += 1\n","\n","\n","cmap = sns.diverging_palette(220, 10, as_cmap=True)\n","u=np.zeros((len(evaluation_list),len(features_name)))\n","CC_heat_map=[shap_importance['training'], shap_importance['validation'], shap_importance['internal_test'], shap_importance['external_test']]\n","\n","for i in range(u.shape[0]):\n","  u[i,:]=CC_heat_map[i]\n","df00=pd.DataFrame(index=evaluation_list,data=u,columns=features_name)\n","fig, ax = plt.subplots(figsize=(16,4))\n","sns.heatmap(df00,cmap=\"Greens\",linewidths=0.5,ax=ax,annot=True,fmt=\".2f\",annot_kws={\"size\": 11, },linewidth=0.5)"],"metadata":{"id":"DGoOOzCtR5Um"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["cmap = plt.get_cmap('rainbow', 4)\n","from itertools import cycle, islice\n","df_shap = pd.DataFrame(shap_importance,features_name )\n","df_shap\n","my_colors = list(islice(cycle([cmap(0), cmap(2), cmap(3), cmap(5)]), None, len(df_shap)))\n","ax=df_shap.plot.bar(width=0.5,capsize=7,color=my_colors)#line()#(width=0.5)\n","ax.set_xticks(np.arange(len(features_name)), labels=features_name)\n","plt.setp(ax.get_xticklabels(), fontsize=8,rotation=35, ha=\"right\",rotation_mode=\"anchor\")\n","plt.show()\n","\n","#ordr_FI_shap=list(sorted(enumerate(importance_permutation), key = itemgetter(1),reverse=True))"],"metadata":{"id":"R27rVTa_uZA-"},"execution_count":null,"outputs":[]}]}